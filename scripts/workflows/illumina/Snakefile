from os.path import join
from samples import Samples
import json

"""
################################################################################
Author: https://github.com/nahanoo
This is the workflow used for sample processing the illumina data.
It aligns the illumina reads to the genome of the wild-type strains used
in a tratment. Those alignment files are analyzed using 
https://github.com/nahanoo/gc_bias/tree/report. The same alignment files
are then used to identify SNPs with https://github.com/tseemann/snippy.
################################################################################
"""

#Setting global data path
work = '/work/FAC/FBM/DMF/smitri/evomicrocomm/genome_size/data/'

def get_strain(wildcards):
    """Because it's easier I shortedned the directory names
    of strains to at, ct, ms and oa. This
    function allows us to back and for between
    abbreviations and full strian name."""
    return Samples().abbreviations[wildcards.strain]


def get_reference(wildcards):
    """Returns reference fasta based on sample name"""
    return join(work,'ancestors',wildcards.strain,'assembly.contigs.polypolish.fasta')


#Mapping illumina reads to wild-type genomes
#Read group in minimap2 command must match the output direcotry in snippy rule
rule mapping:
    input:
        #reference = join(work,'ancestors','{strain}','assembly.contigs.polypolish.fasta'),
        reference = get_reference,
        read1 = join(work,'{sample}','read1.fq.gz'),
        read2 = join(work,'{sample}','read2.fq.gz')
    output:
        join(work,'{sample}','{strain}','mapped_reads.sorted.bam')
    threads:
        8
    shell:
        """
        minimap2 -t 8 -R '@RG\\tID:snippy\\tSM:snippy' -ax sr {input} | \
        samtools view -b -f 3 -q 60 | samtools sort --threads 16 -o {output}
        samtools index {output}
        """



rule split_reads:
    input:
        read1 = join(work,'{sample}','read1.fq.gz'),
        read2 = join(work,'{sample}','read2.fq.gz')
    output:
        join(work,'{sample}','done.txt')
    threads:
        8
    shell:
        """
        python comp_mapping.py {wildcards.sample}
        """

rule split_reads_marc:
    input:
        read1 = join(work,'{sample}','read1.fq.gz'),
        read2 = join(work,'{sample}','read2.fq.gz')
    output:
        join(work,'{sample}','done_marc.txt')
    threads:
        8
    shell:
        """
        python comp_mapping_marc.py {wildcards.sample}
        """

#Getting mapping stats of aligment files.
rule flagstat:
    input:
        join(work,'{sample}','{strain}','mapped_reads.sorted.bam')
    output:
        join(work,'{sample}','{strain}','flagstat.tsv')
    threads:
        2
    shell:
        "samtools flagstat --threads {threads} -O tsv {input} > {output}"

#For creating the report we need to write future plot labels to json
rule labels:
    output:
        join(work,'{sample}','{strain}','labels.json')
    params:
        outdir = join(work,'{sample}','{strain}'),
        strain = get_strain
    threads:
        1
    shell:
        "python create_labels.py {wildcards.sample} {params.strain} {params.outdir}"

rule depth:
    input:
        ref = get_reference,
        bam = join(work,'{sample}','{strain}','mapped_reads.bam')
    output:
        join(work,'{sample}','{strain}','coverage.txt')
    params:
        outdir = join(work,'{sample}','{strain}')
    threads:
        1
    shell:
        "python coverage.py {input.ref} {input.bam} {params.outdir}"

rule samtools_depth:
    input:
        bam = join(work,'{sample}','{strain}','mapped_reads.bam')
    output:
        join(work,'{sample}','{strain}','depth.tsv')
    threads:
        1
    shell:
        "samtools depth -aa -Q 0 {input.bam} > {output}"

rule samtools_depth_marc:
    input:
        bam = join(work,'{sample}','{strain}','mapped_reads_marc.bam')
    output:
        join(work,'{sample}','{strain}','depth_marc.tsv')
    threads:
        1
    shell:
        "samtools depth -aa -Q 0 {input.bam} > {output}"


#Creating repord markdown with
#https://github.com/nahanoo/gc_bias/tree/report
rule report:
    input:
        labels = join(work,'{sample}','{strain}','labels.json'),
        reference = join(work,'ancestors','{strain}','assembly.contigs.polypolish.fasta'),
        bam = join(work,'{sample}','{strain}','mapped_reads.sorted.bam')
    output:
        join(work,'{sample}','{strain}','report.md')
    params:
        outdir = join(work,'{sample}','{strain}')
    threads:
        1
    shell:
        "report_bam_stats {input.reference} {input.bam} {input.labels} {params.outdir}"

#Identifying SNPs using snippy
rule snippy:
    input:
        join(work,'{sample}','done.txt'),
        reference = join(work,'ancestors','{strain}','bakta','snippy.gbff')
    output:
        outfile = join(work,'{sample}','{strain}','snippy','snps.tab')
    threads:
        4
    params:
        outdir = join(work,'{sample}','{strain}','snippy'),
        bam = join(work,'{sample}','{strain}','mapped_reads.bam')
    shell:
        """
        snippy --cpus 4 --force --outdir \
        {params} --reference {input.reference} --bam {params.bam}
        """

rule snippy_core:
    input:
        reference = join(work,'ancestors','{strain}','bakta','assembly.contigs.polypolish.gbff'),
        bam = join(work,'{sample}','{strain}','mapped_reads.filtered.sorted.bam')
    output:
        outfile = join(work,'{sample}','{strain}','snippy-core','snps.tab')
    threads:
        4
    params:
        outdir = join(work,'{sample}','{strain}','snippy-core')
    shell:
        """
        snippy --cpus 4 --force --minfrac 0 --outdir \
        {params} --reference {input.reference} --bam {input.bam}
        """


#Low frequency SNP calling with freebayes
rule freebayes:
    input:
        #reference = join(work,'ancestors','{strain}','assembly.contigs.polypolish.fasta'),
        reference = join(work,'{sample}','reference.fasta'),
        check = join(work,'{sample}','done.txt')
    output:
        outfile = join(work,'{sample}','{strain}','var.vcf')
    params:
        bam = join(work,'{sample}','{strain}','mapped_reads.bam')
    threads:
        1
    shell:
        """
        freebayes -f {input.reference} {params.bam} --min-alternate-count 3 \
        --min-alternate-fraction 0.05 --pooled-continuous --haplotype-length 0 \
        --standard-filters > {output}
        """

rule snpEff:
    input:
        join(work,'{sample}','{strain}','var.vcf')
    output:
        join(work,'{sample}','{strain}','var.annot.vcf')
    threads:
        1
    shell:
        """
        java -Xmx8g -jar ~/apps/snpEff/snpEff.jar {wildcards.strain} {input} > {output}
        """


rule dump_read_ids:
    input:
        d_name = join(work,'{sample}','{strain}','mapped_reads.sorted.bam')
    output:
        join(work,'{sample}','{strain}','read_ids.txt')
    threads:
        1
    shell:
        """
        python dump_read_ids.py {input}
        """

rule filter_bam:
    input:
        bam = join(work,'{sample}','{strain}','mapped_reads.sorted.bam')
    params:

    output:
        join(work,'{sample}','{strain}','mapped_reads.filtered.sorted.bam'),
        join(work,'{sample}','{strain}','mapped_reads.filtered.sorted.bam.bai')
    threads:
        1
    shell:
        """
        python filter_bam.py {wildcards.sample} {input}
        samtools index {output}
        """

rule mapped_reads_only:
    input:
        bam = join(work,'{sample}','{strain}','mapped_reads.filtered.sorted.bam')
    output:
        read1 = join(work,'{sample}','{strain}','read1.fastq.gz'),
        read2 = join(work,'{sample}','{strain}','read2.fastq.gz'),
    threads:
        1
    shell:
        """
        samtools fastq -1 {output.read1} -2 {output.read2} {input}
        """

rule spades:
    input:
        reads = join(work,'{sample}','{strain}','mapped_reads.fastq.gz')
    output:
        assembly = join(work,'{sample}','{strain}','spades','contigs.fasta')
    params:
        out_dir = join(work,'{sample}','{strain}','spades')
    threads:
        1
    shell:
        """
        spades.py --12 {input.reads} --isolate -t {threads}  -o {params.out_dir}
        """

rule trim_read1:
    input:
        read1 = join(work,'{sample}','{strain}','read1.fastq.gz')
    output:
        filtered1 = join(work,'{sample}','{strain}','read1.filtered.fastq.gz'),
    params:
        out_dir = join(work,'{sample}','{strain}')
    threads:
        8
    shell:
        """
        trim-low-abund.py -C 10 -M 2e9 -T {params} --gzip -o {output.filtered1} {input.read1}
        """

rule trim_read2:
    input:
        read2 = join(work,'{sample}','{strain}','read2.fastq.gz')
    output:
        filtered2 = join(work,'{sample}','{strain}','read2.filtered.fastq.gz')
    threads:
        8
    params:
        out_dir = join(work,'{sample}','{strain}')
    shell:
        """
        trim-low-abund.py -C 10 -M 2e9 -T {params} --gzip -o {output.filtered2} {input.read2}
        """
rule sourmash:
    input:
        read1 = join(work,'{sample}','{strain}','read1.filtered.fastq.gz'),
        read2 = join(work,'{sample}','{strain}','read2.filtered.fastq.gz')
    output:
        join(work,'{sample}','{strain}','reads.sig')
    threads:
        1
    shell:
        """
        sourmash sketch dna -p scaled=1000,k=31,k=51 {input} --merge merged -o {output}
        """
